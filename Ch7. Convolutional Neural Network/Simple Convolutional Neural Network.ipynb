{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys, os\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(\"D:\\\\Projects\\\\Python\\\\Deep learning from scratch\")\n",
    "from common.layers import *\n",
    "from common.trainer import Trainer\n",
    "from dataset.mnist import load_mnist\n",
    "from collections import OrderedDict\n",
    "# from "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../img/im2col.png)\n",
    "为了加速计算，将滤波器的应用区域从头开始依次横向展开为1列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col(input_data, filter_h, filter_w, stride = 1, pad = 0):\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride+1\n",
    "    out_w = (W + 2*pad - filter_w)//stride+1\n",
    "    # np.pad [(axis0_begin, axis0_end), (axis1_begin, axis1_end), ...]\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "    \n",
    "    for y in range(filter_h):\n",
    "        y_max = y+stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x+stride*out_w\n",
    "            col[:,:,y,x,:,:] = img[:,:,y:y_max:stride, x:x_max:stride]        \n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1) # transpose 修改多维矩阵轴的顺序\n",
    "    return col\n",
    "\n",
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    N, C, H, W = input_shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n"
     ]
    }
   ],
   "source": [
    "x1 = np.random.rand(1,3,7,7)\n",
    "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
    "print(col1.shape) #9, 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.x = None\n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "        \n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1+ (H + 2*self.pad - FH)/ self.stride)\n",
    "        out_w = int(1+ (W + 2*self.pad - FW)/ self.stride)\n",
    "        \n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T # FN行，-1维元素个数个列\n",
    "        \n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0, 2, 3, 1).reshape(-1, FN)\n",
    "        \n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.x = None\n",
    "        self.arg_max = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H-self.pool_h)/self.stride)\n",
    "        out_w = int(1 + (W-self.pool_w)/self.stride)\n",
    "        \n",
    "        # 展开\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "        \n",
    "        arg_max = np.argmax(col, axis=1)\n",
    "        out = np.max(col, axis=1)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "        return out \n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape+(pool_size,))\n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0]*dmax.shape[1]*dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim=(1, 28, 28),\n",
    "                conv_param={'filter_num':30, 'filter_size':5,'pad':0, 'stride':1},\n",
    "                hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size'] \n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad)/filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2)*(conv_output_size/2))\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0],filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size,hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], \n",
    "                                           self.params['b1'], \n",
    "                                           conv_param['stride'],\n",
    "                                           conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        self.loss(x,t)\n",
    "        dout = 1 \n",
    "        dout = self.last_layer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Conv1'].dW\n",
    "        grads['b1'] = self.layers['Conv1'].db\n",
    "        grads['W2'] = self.layers['Affine1'].dW\n",
    "        grads['b2'] = self.layers['Affine1'].db\n",
    "        grads['W3'] = self.layers['Affine2'].dW\n",
    "        grads['b3'] = self.layers['Affine2'].db\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2996164215540587\n",
      "=== epoch:1, train acc:0.168, test acc:0.155 ===\n",
      "train loss:2.2968998794425555\n",
      "train loss:2.2937523097991805\n",
      "train loss:2.286816685122709\n",
      "train loss:2.2776703730289\n",
      "train loss:2.2692752072598226\n",
      "train loss:2.2486723500106236\n",
      "train loss:2.2289266365153777\n",
      "train loss:2.2074862595199005\n",
      "train loss:2.1803087078981718\n",
      "train loss:2.1215361159009363\n",
      "train loss:2.102650984352325\n",
      "train loss:2.055926198494704\n",
      "train loss:2.0178197130964164\n",
      "train loss:1.9548602548622307\n",
      "train loss:1.8488746878376188\n",
      "train loss:1.8423210252428028\n",
      "train loss:1.7442572901044542\n",
      "train loss:1.6620263762223282\n",
      "train loss:1.5736804943485976\n",
      "train loss:1.5656543894476542\n",
      "train loss:1.4380714660675544\n",
      "train loss:1.4218179702797298\n",
      "train loss:1.1395287686449866\n",
      "train loss:1.224864221304266\n",
      "train loss:1.104967041986366\n",
      "train loss:1.0842407386226927\n",
      "train loss:1.04504162329816\n",
      "train loss:0.9029836832997321\n",
      "train loss:0.8129144325760019\n",
      "train loss:0.8148036948481964\n",
      "train loss:0.9237527381065435\n",
      "train loss:0.9272654916980174\n",
      "train loss:0.6538735718607244\n",
      "train loss:0.7251169362112144\n",
      "train loss:0.7863614326631395\n",
      "train loss:0.741041008699628\n",
      "train loss:0.6898730092680144\n",
      "train loss:0.5176710992129082\n",
      "train loss:0.622024294070382\n",
      "train loss:0.6834040842908524\n",
      "train loss:0.8413827245859844\n",
      "train loss:0.622260766066891\n",
      "train loss:0.49593327968997836\n",
      "train loss:0.4762063140246569\n",
      "train loss:0.6587099441857585\n",
      "train loss:0.6142613682966447\n",
      "train loss:0.583491315045403\n",
      "train loss:0.47079441678145584\n",
      "train loss:0.6116592687400165\n",
      "train loss:0.4747989255934084\n",
      "=== epoch:2, train acc:0.827, test acc:0.815 ===\n",
      "train loss:0.6183217013944897\n",
      "train loss:0.4985350156195698\n",
      "train loss:0.5674136895259461\n",
      "train loss:0.4662534415788446\n",
      "train loss:0.516707129381128\n",
      "train loss:0.49700899989425296\n",
      "train loss:0.5242246639039086\n",
      "train loss:0.4731362957773059\n",
      "train loss:0.46203763488783034\n",
      "train loss:0.4331144593272491\n",
      "train loss:0.4209236781181426\n",
      "train loss:0.3311053193079282\n",
      "train loss:0.4162408130765772\n",
      "train loss:0.33837483398339346\n",
      "train loss:0.4411735457278315\n",
      "train loss:0.5868966608736104\n",
      "train loss:0.39797946233963477\n",
      "train loss:0.4593776584984805\n",
      "train loss:0.6293114601186186\n",
      "train loss:0.4371242196650712\n",
      "train loss:0.4441008686159376\n",
      "train loss:0.5816615708413007\n",
      "train loss:0.3791095937525909\n",
      "train loss:0.45947092412242346\n",
      "train loss:0.31677985021448496\n",
      "train loss:0.39845658382218707\n",
      "train loss:0.2949720711065536\n",
      "train loss:0.3522387132904851\n",
      "train loss:0.32992006201879376\n",
      "train loss:0.3210625868931976\n",
      "train loss:0.34784623204596427\n",
      "train loss:0.5365548072318518\n",
      "train loss:0.3505410154327965\n",
      "train loss:0.44002484294479943\n",
      "train loss:0.21925333808346248\n",
      "train loss:0.27757715738570604\n",
      "train loss:0.31778772114317433\n",
      "train loss:0.24107487085113458\n",
      "train loss:0.2618187323778749\n",
      "train loss:0.2970784838596317\n",
      "train loss:0.3663627683896353\n",
      "train loss:0.3693127913787865\n",
      "train loss:0.3078865617470705\n",
      "train loss:0.25172408253925854\n",
      "train loss:0.3109212679185756\n",
      "train loss:0.357220022643996\n",
      "train loss:0.38783041497611576\n",
      "train loss:0.30305951494325184\n",
      "train loss:0.37264537007391524\n",
      "train loss:0.2628811140376746\n",
      "=== epoch:3, train acc:0.894, test acc:0.871 ===\n",
      "train loss:0.3214304890244947\n",
      "train loss:0.3501553601723665\n",
      "train loss:0.21194513961411496\n",
      "train loss:0.3502580163000979\n",
      "train loss:0.4620667948826484\n",
      "train loss:0.30527541425215876\n",
      "train loss:0.27270800168490067\n",
      "train loss:0.22473289152573714\n",
      "train loss:0.29303973608666706\n",
      "train loss:0.24944273640119716\n",
      "train loss:0.26526910315915125\n",
      "train loss:0.25292864416579636\n",
      "train loss:0.34840344129496326\n",
      "train loss:0.3631061580986548\n",
      "train loss:0.26334727410797704\n",
      "train loss:0.3631622735117257\n",
      "train loss:0.3023591442467938\n",
      "train loss:0.3513596898228753\n",
      "train loss:0.2997028546155422\n",
      "train loss:0.2182802842309011\n",
      "train loss:0.35890221366422453\n",
      "train loss:0.28136512749656595\n",
      "train loss:0.23065743305846997\n",
      "train loss:0.4126507351956644\n",
      "train loss:0.3464922939614618\n",
      "train loss:0.3132164201012532\n",
      "train loss:0.25984982004174895\n",
      "train loss:0.3422944596013967\n",
      "train loss:0.1819212145364872\n",
      "train loss:0.3260943818205022\n",
      "train loss:0.1634821786288076\n",
      "train loss:0.17443745542600747\n",
      "train loss:0.23549355619729295\n",
      "train loss:0.1340394708517444\n",
      "train loss:0.3160389516856679\n",
      "train loss:0.21360422426438258\n",
      "train loss:0.21691677922925287\n",
      "train loss:0.28386722250849794\n",
      "train loss:0.2915778189520111\n",
      "train loss:0.2561139250789499\n",
      "train loss:0.28732901004051287\n",
      "train loss:0.2471738328667624\n",
      "train loss:0.35377129806866475\n",
      "train loss:0.21731725668778054\n",
      "train loss:0.2617938153198739\n",
      "train loss:0.22406774937576188\n",
      "train loss:0.27036038656155903\n",
      "train loss:0.2623733177593019\n",
      "train loss:0.27697326740840705\n",
      "train loss:0.1461672964948205\n",
      "=== epoch:4, train acc:0.906, test acc:0.903 ===\n",
      "train loss:0.23546759264472225\n",
      "train loss:0.14975376705068494\n",
      "train loss:0.1754763730538691\n",
      "train loss:0.1278509022345458\n",
      "train loss:0.1915954768619775\n",
      "train loss:0.18310507089649658\n",
      "train loss:0.36449337362458034\n",
      "train loss:0.18055277237635095\n",
      "train loss:0.32641086806258457\n",
      "train loss:0.5242557985890923\n",
      "train loss:0.12933887142033604\n",
      "train loss:0.1792797172910199\n",
      "train loss:0.17373272199603507\n",
      "train loss:0.29805957492354024\n",
      "train loss:0.17302488453714582\n",
      "train loss:0.22961332363715492\n",
      "train loss:0.2759029602163526\n",
      "train loss:0.1693202667497827\n",
      "train loss:0.19466729153775553\n",
      "train loss:0.20299636193662052\n",
      "train loss:0.3818649311181808\n",
      "train loss:0.22614422254881197\n",
      "train loss:0.39664292412067753\n",
      "train loss:0.19841471019042903\n",
      "train loss:0.2626503053764061\n",
      "train loss:0.18542963085029038\n",
      "train loss:0.1389537765533315\n",
      "train loss:0.3250065228302065\n",
      "train loss:0.3676060829089988\n",
      "train loss:0.353097751477402\n",
      "train loss:0.3316783419145719\n",
      "train loss:0.2752093881466585\n",
      "train loss:0.1467509433380389\n",
      "train loss:0.31813596830807644\n",
      "train loss:0.18068005266891926\n",
      "train loss:0.20054232299832106\n",
      "train loss:0.30573372657981407\n",
      "train loss:0.33682073318745714\n",
      "train loss:0.1941716485387721\n",
      "train loss:0.3096720103770114\n",
      "train loss:0.33968975474284946\n",
      "train loss:0.1893235855860607\n",
      "train loss:0.2227403470355775\n",
      "train loss:0.21266955040237895\n",
      "train loss:0.41473329954898364\n",
      "train loss:0.21712365528010852\n",
      "train loss:0.1850261175277964\n",
      "train loss:0.2321505498688085\n",
      "train loss:0.3450635956844071\n",
      "train loss:0.23374290361807198\n",
      "=== epoch:5, train acc:0.922, test acc:0.901 ===\n",
      "train loss:0.25557603403932366\n",
      "train loss:0.18081194626264163\n",
      "train loss:0.1722934115708689\n",
      "train loss:0.19752277008330282\n",
      "train loss:0.20525505622590287\n",
      "train loss:0.261218276393803\n",
      "train loss:0.29585099798981684\n",
      "train loss:0.22282882361313813\n",
      "train loss:0.13161420198140975\n",
      "train loss:0.24405561498849718\n",
      "train loss:0.17388011421456248\n",
      "train loss:0.17567775605334646\n",
      "train loss:0.1510308410847772\n",
      "train loss:0.29712381383329683\n",
      "train loss:0.10932870032217469\n",
      "train loss:0.2639834944777506\n",
      "train loss:0.2810962638883279\n",
      "train loss:0.1368754402095903\n",
      "train loss:0.23677538752254484\n",
      "train loss:0.21321360882488125\n",
      "train loss:0.1659713285601451\n",
      "train loss:0.16937589070011325\n",
      "train loss:0.20603289815714956\n",
      "train loss:0.12732007658598332\n",
      "train loss:0.08962173525115008\n",
      "train loss:0.26712075226451576\n",
      "train loss:0.15845681461077252\n",
      "train loss:0.1438120190557445\n",
      "train loss:0.1406474684517089\n",
      "train loss:0.13852996647514704\n",
      "train loss:0.1801072069211983\n",
      "train loss:0.1953403606508053\n",
      "train loss:0.2412537084416756\n",
      "train loss:0.3585444853763297\n",
      "train loss:0.11727730464210065\n",
      "train loss:0.11799752967290358\n",
      "train loss:0.11030478591789099\n",
      "train loss:0.10503719381222158\n",
      "train loss:0.14584968620195837\n",
      "train loss:0.16349416790035307\n",
      "train loss:0.17431619059682643\n",
      "train loss:0.26109715942831735\n",
      "train loss:0.20371965055818955\n",
      "train loss:0.23816923610946347\n",
      "train loss:0.16649569262637867\n",
      "train loss:0.06325864276132084\n",
      "train loss:0.1340456103066741\n",
      "train loss:0.13604080979587718\n",
      "train loss:0.1711517616436663\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.922\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 读入数据\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 处理花费时间较长的情况下减少数据 \n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 5\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 保存参数\n",
    "# network.save_params(\"params.pkl\")\n",
    "# print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 绘制图形\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
