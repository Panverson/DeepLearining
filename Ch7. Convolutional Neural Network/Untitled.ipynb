{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys, os\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(\"D:\\\\Projects\\\\Python\\\\Deep learning from scratch\")\n",
    "from common.layers import *\n",
    "from common.trainer import Trainer\n",
    "from dataset.mnist import load_mnist\n",
    "from collections import OrderedDict\n",
    "# from "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../img/im2col.png)\n",
    "为了加速计算，将滤波器的应用区域从头开始依次横向展开为1列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col(input_data, filter_h, filter_w, stride = 1, pad = 0):\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride+1\n",
    "    out_w = (W + 2*pad - filter_w)//stride+1\n",
    "    # np.pad [(axis0_begin, axis0_end), (axis1_begin, axis1_end), ...]\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "    \n",
    "    for y in range(filter_h):\n",
    "        y_max = y+stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x+stride*out_w\n",
    "            col[:,:,y,x,:,:] = img[:,:,y:y_max:stride, x:x_max:stride]        \n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1) # transpose 修改多维矩阵轴的顺序\n",
    "    return col\n",
    "\n",
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    N, C, H, W = input_shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n"
     ]
    }
   ],
   "source": [
    "x1 = np.random.rand(1,3,7,7)\n",
    "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
    "print(col1.shape) #9, 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.x = None\n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "        \n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1+ (H + 2*self.pad - FH)/ self.stride)\n",
    "        out_w = int(1+ (W + 2*self.pad - FW)/ self.stride)\n",
    "        \n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T # FN行，-1维元素个数个列\n",
    "        \n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0, 2, 3, 1).reshape(-1, FN)\n",
    "        \n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.x = None\n",
    "        self.arg_max = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H-self.pool_h)/self.stride)\n",
    "        out_w = int(1 + (W-self.pool_w)/self.stride)\n",
    "        \n",
    "        # 展开\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "        \n",
    "        arg_max = np.argmax(col, axis=1)\n",
    "        out = np.max(col, axis=1)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "        return out \n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape+(pool_size,))\n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0]*dmax.shape[1]*dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim=(1, 28, 28),\n",
    "                conv_param={'filter_num':30, 'filter_size':5,'pad':0, 'stride':1},\n",
    "                hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size'] \n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad)/filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2)*(conv_output_size/2))\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0],filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size,hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], \n",
    "                                           self.params['b1'], \n",
    "                                           conv_param['stride'],\n",
    "                                           conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        self.loss(x,t)\n",
    "        dout = 1 \n",
    "        dout = self.last_layer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Conv1'].dW\n",
    "        grads['b1'] = self.layers['Conv1'].db\n",
    "        grads['W2'] = self.layers['Affine1'].dW\n",
    "        grads['b2'] = self.layers['Affine1'].db\n",
    "        grads['W3'] = self.layers['Affine2'].dW\n",
    "        grads['b3'] = self.layers['Affine2'].db\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.299676735296286\n",
      "=== epoch:1, train acc:0.261, test acc:0.244 ===\n",
      "train loss:2.2979658805803966\n",
      "train loss:2.2934772485397947\n",
      "train loss:2.2844454299695234\n",
      "train loss:2.2773482388331887\n",
      "train loss:2.264950367223014\n",
      "train loss:2.244242027282779\n",
      "train loss:2.2370944216414785\n",
      "train loss:2.22786431923229\n",
      "train loss:2.189550731596025\n",
      "train loss:2.1171374106719547\n",
      "train loss:2.1563883807593496\n",
      "train loss:2.0943543605129467\n",
      "train loss:2.062788147884417\n",
      "train loss:2.0259183259603986\n",
      "train loss:1.9386284609606088\n",
      "train loss:1.8506024617890988\n",
      "train loss:1.7720484656910636\n",
      "train loss:1.7201021221941541\n",
      "train loss:1.6786696726966404\n",
      "train loss:1.5671730593924358\n",
      "train loss:1.4156199470485453\n",
      "train loss:1.3493833869534426\n",
      "train loss:1.304802696232291\n",
      "train loss:1.303675432112101\n",
      "train loss:1.153084602399324\n",
      "train loss:1.0976837394736767\n",
      "train loss:1.049890722792564\n",
      "train loss:1.0261582875071702\n",
      "train loss:0.8921746857937981\n",
      "train loss:0.957074575835355\n",
      "train loss:0.8346408203360604\n",
      "train loss:0.6265014718112699\n",
      "train loss:0.8808032673317308\n",
      "train loss:0.8425473229333008\n",
      "train loss:0.5770576657845257\n",
      "train loss:0.6812490281054033\n",
      "train loss:0.8428235511396035\n",
      "train loss:0.7758204475759733\n",
      "train loss:0.7797072228067146\n",
      "train loss:0.6995676022268325\n",
      "train loss:0.8127977158021108\n",
      "train loss:0.49002250904862515\n",
      "train loss:0.6470817400463548\n",
      "train loss:0.6408783137042788\n",
      "train loss:0.6065684070256715\n",
      "train loss:0.40036798653296224\n",
      "train loss:0.4441895787844255\n",
      "train loss:0.40690365906696696\n",
      "train loss:0.3771632356952661\n",
      "train loss:0.5253183681133978\n",
      "=== epoch:2, train acc:0.812, test acc:0.795 ===\n",
      "train loss:0.6398358790622092\n",
      "train loss:0.5189593143946054\n",
      "train loss:0.48658558588859724\n",
      "train loss:0.42068414516725544\n",
      "train loss:0.42296476306556974\n",
      "train loss:0.4122224175217239\n",
      "train loss:0.38987565574475136\n",
      "train loss:0.4609150714781999\n",
      "train loss:0.43555651787458144\n",
      "train loss:0.3264163516633019\n",
      "train loss:0.39910341498637486\n",
      "train loss:0.486499669909158\n",
      "train loss:0.45989378591054647\n",
      "train loss:0.44999956445245337\n",
      "train loss:0.4454041818591066\n",
      "train loss:0.47290550217858046\n",
      "train loss:0.47753236224617474\n",
      "train loss:0.3580396455639649\n",
      "train loss:0.3510357309223467\n",
      "train loss:0.5046253014484805\n",
      "train loss:0.2920304740060405\n",
      "train loss:0.3437009554397868\n",
      "train loss:0.27749479011354083\n",
      "train loss:0.37433748774750375\n",
      "train loss:0.39656259550142425\n",
      "train loss:0.352439424780972\n",
      "train loss:0.3516282086246227\n",
      "train loss:0.4961455748567556\n",
      "train loss:0.4369491275078773\n",
      "train loss:0.41696303144335145\n",
      "train loss:0.4520606166357554\n",
      "train loss:0.3539683717920451\n",
      "train loss:0.4159017837589908\n",
      "train loss:0.6059056139511964\n",
      "train loss:0.2632707442110974\n",
      "train loss:0.483645895650747\n",
      "train loss:0.33512323767119717\n",
      "train loss:0.2969268363041538\n",
      "train loss:0.5034554442521325\n",
      "train loss:0.2807600957254597\n",
      "train loss:0.36907797960731487\n",
      "train loss:0.3361427198533861\n",
      "train loss:0.3033954586339472\n",
      "train loss:0.5626443997069144\n",
      "train loss:0.3340143994934538\n",
      "train loss:0.5176417014971726\n",
      "train loss:0.2508714733398233\n",
      "train loss:0.271835064680677\n",
      "train loss:0.19444197032612112\n",
      "train loss:0.31927145453339567\n",
      "=== epoch:3, train acc:0.863, test acc:0.866 ===\n",
      "train loss:0.310045848572164\n",
      "train loss:0.3946463048254686\n",
      "train loss:0.5031364169062089\n",
      "train loss:0.3264754342681956\n",
      "train loss:0.378097734829533\n",
      "train loss:0.23957579156986405\n",
      "train loss:0.2898736588205501\n",
      "train loss:0.29869760514099347\n",
      "train loss:0.2788035011073678\n",
      "train loss:0.41808133830441496\n",
      "train loss:0.2319887548076583\n",
      "train loss:0.35039666353541926\n",
      "train loss:0.28588527575077366\n",
      "train loss:0.39671111954102467\n",
      "train loss:0.2002224969427138\n",
      "train loss:0.23470904878553384\n",
      "train loss:0.38821760302634983\n",
      "train loss:0.42722565470560453\n",
      "train loss:0.28783272638048957\n",
      "train loss:0.32147004023802267\n",
      "train loss:0.31551449309591834\n",
      "train loss:0.33283324520166063\n",
      "train loss:0.3854269868749817\n",
      "train loss:0.4034193713301918\n",
      "train loss:0.3495407102497611\n",
      "train loss:0.3106505099796686\n",
      "train loss:0.2889892581657667\n",
      "train loss:0.3505509729122551\n",
      "train loss:0.26577294230825016\n",
      "train loss:0.2017934917887925\n",
      "train loss:0.32208302212240997\n",
      "train loss:0.31914867073359904\n",
      "train loss:0.35789958975215347\n",
      "train loss:0.20506507072299257\n",
      "train loss:0.32186534608691597\n",
      "train loss:0.32740389481099347\n",
      "train loss:0.33277223161388114\n",
      "train loss:0.34592160516171105\n",
      "train loss:0.25236085746996734\n",
      "train loss:0.4114519558144586\n",
      "train loss:0.3343552965989177\n",
      "train loss:0.339321958314278\n",
      "train loss:0.19516362483499833\n",
      "train loss:0.35022832807674503\n",
      "train loss:0.2518685345657201\n",
      "train loss:0.2583053804495734\n",
      "train loss:0.2590409930231262\n",
      "train loss:0.32977749605565526\n",
      "train loss:0.37341981063164453\n",
      "train loss:0.21446955923822805\n",
      "=== epoch:4, train acc:0.883, test acc:0.887 ===\n",
      "train loss:0.27810645713492377\n",
      "train loss:0.23160295386274915\n",
      "train loss:0.37661900793579817\n",
      "train loss:0.2418623332322372\n",
      "train loss:0.4318498151255226\n",
      "train loss:0.34590778390848675\n",
      "train loss:0.28877438347233686\n",
      "train loss:0.43371315264354765\n",
      "train loss:0.22620618288460922\n",
      "train loss:0.4092641702439483\n",
      "train loss:0.43506380538024764\n",
      "train loss:0.28453555340056835\n",
      "train loss:0.28546683388069544\n",
      "train loss:0.3008760021375527\n",
      "train loss:0.2908784132324332\n",
      "train loss:0.2424752610586922\n",
      "train loss:0.264052213539884\n",
      "train loss:0.3462458614241017\n",
      "train loss:0.18178336319841992\n",
      "train loss:0.20407661788233178\n",
      "train loss:0.3051773303691734\n",
      "train loss:0.4049652383845534\n",
      "train loss:0.2880582781752768\n",
      "train loss:0.2591981427489069\n",
      "train loss:0.23133757332525218\n",
      "train loss:0.45017551336799877\n",
      "train loss:0.3783972318566217\n",
      "train loss:0.29051912604586194\n",
      "train loss:0.31601934279923416\n",
      "train loss:0.2556170468369789\n",
      "train loss:0.18381525202202512\n",
      "train loss:0.19271034529424522\n",
      "train loss:0.1219207277280227\n",
      "train loss:0.30486093326349334\n",
      "train loss:0.1565336187374514\n",
      "train loss:0.17702203514315815\n",
      "train loss:0.4500804800002704\n",
      "train loss:0.2064334977721286\n",
      "train loss:0.29909094697491834\n",
      "train loss:0.17649565388053914\n",
      "train loss:0.18334388512018246\n",
      "train loss:0.23104829773464897\n",
      "train loss:0.28241637930684454\n",
      "train loss:0.30418700029579304\n",
      "train loss:0.2835993526726931\n",
      "train loss:0.24610160859876415\n",
      "train loss:0.15621245942036205\n",
      "train loss:0.38002491147734446\n",
      "train loss:0.3322529312380425\n",
      "train loss:0.32749169527335126\n",
      "=== epoch:5, train acc:0.919, test acc:0.904 ===\n",
      "train loss:0.25490795751377865\n",
      "train loss:0.14977751623765387\n",
      "train loss:0.1791740783958179\n",
      "train loss:0.17350967885657947\n",
      "train loss:0.3275715529663984\n",
      "train loss:0.12444440209261318\n",
      "train loss:0.2852614064376449\n",
      "train loss:0.2973435025303221\n",
      "train loss:0.1934389988376062\n",
      "train loss:0.1927172706313274\n",
      "train loss:0.23914962873385862\n",
      "train loss:0.1776241730243895\n",
      "train loss:0.15954863694338337\n",
      "train loss:0.2150263391990469\n",
      "train loss:0.27375471867475837\n",
      "train loss:0.22833884517234473\n",
      "train loss:0.24898320451386272\n",
      "train loss:0.3278444840626267\n",
      "train loss:0.29922610671198335\n",
      "train loss:0.1755417125431945\n",
      "train loss:0.2381538115358163\n",
      "train loss:0.16148523696767156\n",
      "train loss:0.18862870913042845\n",
      "train loss:0.23219014016519193\n",
      "train loss:0.1803420308849943\n",
      "train loss:0.2513169157373134\n",
      "train loss:0.15987504149652396\n",
      "train loss:0.19203524528271043\n",
      "train loss:0.17716447478066896\n",
      "train loss:0.3252793405341904\n",
      "train loss:0.21434279069486942\n",
      "train loss:0.12453748764608179\n",
      "train loss:0.18102059395974504\n",
      "train loss:0.24240851808723818\n",
      "train loss:0.26757760050135554\n",
      "train loss:0.1589516812096866\n",
      "train loss:0.23733465310334784\n",
      "train loss:0.15575424820149178\n",
      "train loss:0.2259014892569614\n",
      "train loss:0.16548985114974424\n",
      "train loss:0.2836777152102375\n",
      "train loss:0.1608635343204347\n",
      "train loss:0.14521665584596546\n",
      "train loss:0.28642859825020206\n",
      "train loss:0.31396020508814076\n",
      "train loss:0.23485557214570385\n",
      "train loss:0.16566187372057314\n",
      "train loss:0.2754426715632314\n",
      "train loss:0.20215005777383196\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.921\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SimpleConvNet' object has no attribute 'save_params'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-4f251ea7a904>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# 保存参数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"params.pkl\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Saved Network Parameters!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SimpleConvNet' object has no attribute 'save_params'"
     ]
    }
   ],
   "source": [
    "# 读入数据\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 处理花费时间较长的情况下减少数据 \n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 5\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 保存参数\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 绘制图形\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
